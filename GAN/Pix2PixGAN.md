图像处理的很多问题都是将一张输入的图片转变为一张对应的输出图片，比如灰度图、彩色图之间的转换、图像自动上色等。Pix2Pix就是在cGAN基础上升级的，只是条件变成了图片而不再是一个简单的标签。

# 改进
普通的GAN接收的G部分（生成器）的输入是随机向量，输出是图像。
D部分（判别器）接收的输入是图像(生成的或是真实的)，输出是对或者错这样G和D联手就能输出真实的图像。
Pix2pix GAN本质上是一个CGAN，图片x作为此cGAN的条件需要输入到G和D中。

# Pix2Pix生成器设计
如果使用普通的卷积神经网络，那么会导致每一层都承载保存着所有的信息，这样神经网络很容易出错。
在Pix2Pix中，是使用U-Net模型。

![[Pasted image 20231222140824.png|410]](./images/20231222140824.png)
![[Pasted image 20231222133400.png|410]](./images/20231222133400.png)


# Pix2Pix判别器设计
但是D的输入却应该发生一些变化，因为除了要生成真实图像之外，还要保证生成的图像和输入图像是匹配的。
于是D的输入就做了一些变动。D中要输入成对的图像。这类似于[[条件生成对抗网络(CGAN)]]。
在Pix2Pix论文中，判别器实现为 **Patch-D** ，所谓Patch，指无论生成的图像有多大，将其切分为多个固定大小的Patch输入进D去判断。
pix2pix是先将图像打成N×N的patches，再将每个patch送到鉴别器中进行判别，最后取得判别的均值作为最终结果。

## Patch discriminator的原因
相当于是对图像空间/分布进行进一步的细化，这使得输入与输出的图像分布可以更进一步、更细化地拟合，从而得到更好的效果。



# 损失函数

D网络损失函数（判别器）

- 输入真实的成对图像希望判定为1
- 输入生成图像与原图像希望判定为0

G网络损失函数（生成器）

- 输入生成图像与原图像希望判定为1

增加了L1的loss之后，最终的损失函数如下图所示：



$$G^*=\arg\min\limits_G\max\limits_D\mathcal{L}_{cGAN}(G,D)+\lambda\mathcal{L}_{L1}(G)$$

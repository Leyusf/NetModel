GAN（生成对抗网络）是一种用于生成逼真数据的机器学习模型。它由两个主要组件组成: **生成器(Generator)** 和 **判别器(Discriminator)**。

**生成器** 负责生成伪造的数据样本，而 **判别器** 则负责评估这些样本的真实性。

生成网络产生“假”数据，并试图欺骗判别网络判别网络对生成数据进行真伪鉴别，试图正确识别所有假数据。在训练迭代的过程中，两个网络持续地进化和对抗直到达到平衡状态(参考纳什均衡)，判别网络无法再识别假”数据，训练结束。

生成器主要用来学习真实图像分布从而让自身生成的图像更加真实，以骗过判别器，判别器则需要对接收的图片进行真假判别。

## 训练
在训练过程中，生成器努力地让生成的图像更加真实而判别器则努力地去识别出图像的真假，这个过程相当于一个二人博弈，随着时间的推移，生成器和判别器在不断地进行对抗。
最终两个网络达到了一个动态均衡:生成器生成的图像接近于真实图像分布，而判别器识别不出真假图像对于给定图像的预测为真的概率基本接近0.5(相当于随机猜测类别)。

## 设计
GAN设计的关键在于**损失函数的处理**。
对于**判别模型**，损失函数是容易定义的，判别器主要用来判断一张图片是真实的还是生成的，显然这是一个二分类问题。
对于**生成模型**，损失函数的定义就不是那么容易我们希望生成器可以生成接近真实的图片，对于生成的图片是否像真实的，我们人类肉眼容易判断，但具体到代码中，往往是一个抽象的，难以数学公理化定义的范式。
判别模型和生成模型是MLP的话，可以使用反向传播来训练，而不需要使用马尔科夫链。
生成模型是一个MLP可以将噪声的分布映射成任何一个想要拟合的分布。
判别模型是一个二分类模型。


![[Pasted image 20231201124212.png|410]](./images/20231201124212.png)

## 原理
生成模型学习真实数据 $x$ 的分布 $p_g$ 。
定义一个先验的噪音变量分布 $p_z(z)$ , 生成模型将 $z$ 映射成 $x$ ,生成模型是一个MLP表示为 $G(z, \theta_g)$ ,表示 $G$ 接受噪音变量 $z$ 学习参数 $\theta_g$ 。不在乎原始分布，只使用MLP来近似，可以快速计算。
判别模型 $D(x, \theta_d)$ 接受输入 $x$ 学习参数 $\theta_d$ 输出判别结果。训练 $D$ 的 **同时** 训练 $G$。
$G$ 被训练使得 $log(1-D(G(z)))$ 最小。
此处，在 $G$ 效果很好时，$D$ 的输出是1 ( $D$ 判别所有输入都来自真的图片), 所以 $log(1-D(G(z)))=log(1-1)=\infty$ 。
若 $G$ 的效果非常不好时， $D$ 的输出是0, 所以 $log(1-D(G(z)))=log(1)=0$ 。

![[Pasted image 20231201131238.png|410]](./images/20231201131238.png)

$D(x)= \{{0 , x \in P(z) \atop 1 , x \in P(x)}$ 

换而言之， $D$ 和 $G$ 在玩两个人的 minmax 游戏，表示为 $V(G,D)$ 。

$$
\min_G \max_D V(G, D) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

$\mathbb{E}_{x \sim p_{\text{data}}(x)}$ 表示真实数据分布的期望。
$\mathbb{E}_{z \sim p_z(z)}$ 表示噪音的分布。

第一项 $log(D(x))$ 表示判别器的性能，我们希望 $D$ 是完美的，所以希望 $log(D(x))$ 最大以至于 $log(D(x))=log(1)=0$ 这里的 $x$ 来自于真实图片分布。

第二项这里的 $z$ 来自噪音分布。
对于 $D$ 来说，输入是生成数据，希望输出是0，所以希望 $log(1-D(G(z)))$ 则  $log(1 - D(G(z)))=log(1-0)=0$ 。
对于 $G$ 来说，希望能骗过 $D$，则 $D(G(z))$ 越大越好，所以 $log(1-D(G(z)))$ 越小越好， $log(1-D(G(z)))=log(1-1)=\infty$ 。

判别器 $D$ 和生成器 $G$ 对于 $log(1-D(G(z)))$ 的优化目标是相反的。
对于 $G$ 取最小值，对于 $D$ 取最大值。

总结起来：
对于判别器 $D$ ,我们希望最大化 $D(x)$ 和 $log⁡(1-D(G(z)))$ ,从而达到最大化 $V(D, G)$ 的目标；
对于生成器 $G$ ,其训练目标是最小化 $log⁡(1-D(G(z)))$ ,从而达到最小化 $V(D, G)$ 的目标。
一般使用**相似度**作为注意力分数，即更注意和query相似的key。
注意力评分函数$a$将$q$ 和$k$映射为标量再进行softmax归一化，可以看作是相似度。
$\alpha(q,k_i)=softmax(a(q,k_i))v_i$
选择不同的注意力评分函数$a$会导致不同的[[注意力汇聚]]操作。

Attention的计算过程。
![[Pasted image 20231020192933.png|575]]

## 加性注意力(Additive attention)

一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。 
$a(q, k_i) = {W_v}^Ttanh(W_kk_i+W_qq)$
其中可学习的参数有$W_v$、$W_k$和$W_q$。将查询$q$和键$k$连结起来后输入到一个多层感知机（MLP）中， 感知机包含一个隐藏层，其隐藏单元数是一个超参数$ℎ$。 通过使用$tanh$作为激活函数，并且禁用偏置项。

## 缩放点积注意力(scaled dot-product attention)

使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同的长度$d$。假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为0，方差为$d$。为确保无论向量长如何，点积的方差在不考虑向量长度的情况下仍然是1， 我们再将点积除以$\sqrt d$。

$a(q,k)=q^Tk / \sqrt d$
$\alpha(Q, K)=softmax({QK^T \over \sqrt d})V$

因为向量点积等于余弦相似度，所以缩放点积注意力分数可以理解为余弦相似度除以一个缩放值。
$a(q,k)=q^Tk / \sqrt d = cosine(q, k) / \sqrt d$
$\alpha(Q, K)=softmax({cosine(Q, K) \over \sqrt d})V$





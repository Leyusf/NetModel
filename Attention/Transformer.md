基于[[编码器-解码器]]的架构来处理序列对。Transformer模型完全基于[[注意力机制]]，没有任何卷积层或[[循环神经网络(RNN)]]层。
Transformer的编码器和解码器是基于自注意力的模块叠加而成的，**源(输入)** 序列和**目标(输出)** 序列的**嵌入(embedding)** 表示将加上[[自注意力和位置编码|位置编码]](positional encoding) ，再分别输入到编码器和解码器中。

![[Pasted image 20231027201859.png|505]](../images/20231027201859.png)

Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层。第一个子层是[[多头注意力机制]]多头自注意力(multi-head self-attention) 汇聚；第二个子层是**基于位置的前馈网络(positionwise feed-forward network)**。
具体来说，在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出。每个子层都采用了**残差连接**。
**逐位前馈网络**是一个全连接网络。

## 基于位置的前馈网络
- 将输入形状由 $(b, n, d)$ 变为 $(bn, d)$ ，其中b是batch size，n是序列长度，d是dimension。
- 作两个全连接。
- 输出形状由 $(bn,d)$ 变为 $(b,n,d)$ 。
- 等价于两个 $1 \times 1$ 的一维卷积层。

## LayerNorm(层归一化)
批量归一化(BatchNorm)对每个特征/通道里元素进行归一化，但是不适用序列长度会变的情况。
层归一化(LayerNorm)对每个样本的元素进行归一化。

![[1698411270884.png]](../images/1698411270884.png)

可以理解为：
BN对同一特征的不同样本做归一化。
LN对同一样本的所有特征做归一化。

## 解码器
1. 对输入的embedding加上位置编码，得到X。
2. 将X与其通过多头注意力的结果相加并进行LayerNorm，得到X。
3. 将X与通过PFFN的结果相加并进行LayerNorm，得到H。

## 编码器
编码器中的[[多头注意力机制|多头注意力]]是带有掩码的多头注意力。
因为，解码器对序列中一个元素输出时，不应该考虑该元素之后的元素，所以通过掩码，计算 $x_i$ 输出时，假装当前序列长度为 $i$ 。

## 信息传递
编码器的输出 $y_1,...,y_n$
将其作为解码器中第 $i$ 个多头注意力的key和value，它的query来自目标序列。
意味着编码器和解码器中块的个数和输出维度是一样的。

## 预测
再预测第 $t+1$ 个输出时，解码器中输入前 $t$ 个预测值，在自注意力中，前 $t$ 个预测值作为key和value，第 $t$ 个预测值还作为query。
![[1 1.png|490]]


在Seq2Seq结构中，encoder把所有的输入序列都编码成一个统一的语义向量Context(我们在解码器中使用的是编码器的最后一个隐状态)，然后再由Decoder解码。由于context包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。
如机器翻译问题，当要翻译的句子较长时，一个Context可能存不下那么多信息，就会造成精度的下降。除此之外，如果按照上述方式实现，只用到了编码器的最后一个隐藏层状态，信息利用率低下。

所以如果要改进[[Seq2Seq]]结构，最好的切入角度就是：利用**Encoder所有隐藏层 $h$ 状态**解决Context长度限制问题。


![[Pasted image 20231021173921.png|580]](../images/20231021173921.png)
在使用注意力机制的Seq2Seq模型模型中，解码器接受的上下文(原来是编码器最后一个隐状态)变为编码器中原单词的隐状态。例如:


![[Pasted image 20231021174234.png]](../images/20231021174234.png)
- 此时，编码器每次词的 **输出(隐状态)** 作为Key和Value。
- 解码器RNN对上一个词的 **输出(隐状态)** 是Query。
- 注意力输出和下一个词的词嵌入合并进入下一轮解码器。


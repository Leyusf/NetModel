在深度学习中，经常使用卷积神经网络（CNN）或[[循环神经网络(RNN)]]对序列进行编码。 想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 _自注意力_（self-attention）。

**自注意力模型**的每个输出基于**全局信息**，并且可以**并行化**计算。
将 $x_i$ 当作key、value、query对序列抽取特征 $y_i$ 。

$y_i=f(x_i,(x_1,x_1),(x_2,x_2),...,(x_n,x_n))$

## 自注意力机制计算过程

(1) 计算QKV：
假设输入序列为 $X=[x_1,x_2,...x_N] \in R^{D_x \times N}$ ，经过**词嵌入**得到 $A=[a_q,...,a_N] \in R^{D_a \times N}$ ，将词嵌入矩阵映射到三个不同的空间得到:

* $Q=[q_1,...,q_N] \in R^{D_k \times N}$
* $K = [k_1,...,k_N] \in R^{D_k \times N}$
* $V=[v_1,...,v_N] \in R^{D_v \times N}$

![[Pasted image 20231021222433.png]](../images/20231021173921.png)
QKV是相同的。
矩阵运算如下：

$$Q=W_qA, W_q \in R^{D_k \times D_a}$$

$$K=W_kA, W_k \in R^{D_k \times D_a}$$

$$V=W_vA, W_v \in R^{D_v \times D_a}$$


(2)计算注意力权重
对于每个查询向量 $q_i$ 使用键值对注意力机制，得到注意力权重(Attention Weights) $a_1,...a_N$ 。

(3)加权求和
根据注意力权重，计算注意力值(Attention):

$$B = AV$$

自注意力模型的优点：
提高并行计算效率。
捕捉长距离的依赖关系。

## 位置编码(Position Encoding)
自注意力机制忽略了序列 $X=[x_1,x_2,...x_N] \in R^{D_x \times N}$ 中每个 $x$ 的位置信息，即打乱序列顺序不影响输出结果。因此在模型中显示的引入**位置编码 $e$ **。

![[Pasted image 20231021223302.png|261]](../images/20231021223302.png)


位置编码 $e$ 是自注意力机制中获取序列位置信息的唯一来源，是模型重要的组成部分。位置编码既可以从数据中学习得到，也可以人为定义并加到词嵌入向量上。
位置编码的定义如下:

$$e_{i,2j}=sin({i \over 10000^{2j/d}})$$
$$e_{i,2j+1}=cos({i \over 10000^{2j/d}})$$






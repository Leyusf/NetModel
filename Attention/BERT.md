基于微调的NLP模型。
预训练的模型抽取了足够多的信息。
新的任务只需要增加一个简单的输出层。

## 架构
- 只有[[编码器-解码器|编码器]]的[[Transformer]]。
- 两个版本：
	- Base: Blocks=12, hidden size=768, heads=12, parameters=110M
	- Large: Blocks=24, hidden size=1024, heads=16, parameters=340M
- 在大规模数据上训练 > 3B 词

## 训练
- 对输入的修改:
	- 每个样本是一个句子对。
	- 加入额外的片段嵌入。
	- 位置编码科学。
	- BERT输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。

![[Pasted image 20231031120756.png|700]](../images/20231031120756.png)

- 预训练任务1：带掩码的语言模型
	- Transfomer的编码器是双向的，标准语言模型是单向的。
	- 带掩码的语言模型每次随机(15%)将一些词元换成 ` <mask> ` 。可以理解为完形填空。
	- 因为微调任务中不出现 ` <mask> ` 所有
		- 80%，将选择词元换成 ` <mask> ` 。
		- 10%，换成下一次随机词元。
		- 10%保持原有词元。
- 预训练任务2：下一句子预测
	- 预测一个句子对中的两个句子是不是相邻的。
	- 训练样本中：
		- 50%选择相邻的句子对
		- 50%选择随机句子对
	- 将 ` <cls> ` 对应的输出放到全连接层来预测

## 微调
- BERT对每个词元返回抽取了上下文信息的特征向量。
- 不同的任务使用不同的特性。

### 句子分类
- 将 ` <cls> ` 对应的向量输入到全连接层分类。

![[Pasted image 20231102154449.png|406]](../images/20231102154449.png)

做单句子的分类(预测是正面评价还是负面评价), 只取cls的向量，其余的向量不要，再进行分类。

![[Pasted image 20231102154501.png|431]](../images/20231102154501.png)

做一对句子，也只使用cls的向量做分类。

### 命名实体识别
- 识别一个词元是不是命名实体，例如人名、机构、位置。
- 将非特殊词元(不是cls、sep的词元)放进全连接层分类。

![[Pasted image 20231102154920.png|550]](../images/20231102154920.png)

把每一个词元进行二分类判断。

### 问题回答
- 给定一个问题喝文字描述，找出一个片段作为回答。
- 对片段中的每个次元预测它是不是回答的开头或结束。

![[Pasted image 20231102155205.png|506]](../images/20231102155205.png)

对每一个文章(描述)的词元去预测时开始、结束或者不是答案。

## 总结
BERT是针对微调的

基于Transfomer的编码器做了修改：
模型更大，训练数据更多
输入句子对，片段嵌入，可学习的位置编码
训练时的两个任务：
- 带掩码的语言模型
- 下一个句子预测

即使下游任务各有不同，使用BERT微调时均只需要增加输出层，但是根据任务的不同，输入的表示和使用的VERT特征也会不一样。

文本是最常见的具有时间序列的数据，其可以被简单的视作一串**单词**序列或者**字符**序列。
解析文本的常见预处理步骤通常包括：

1. 将文本作为字符串加载到内存中。
2. 将字符串拆分为**词元(token)**，词元是单词或者字符。
3. 建立一个**词表**，将拆分的词元映射到数字索引。
4. 将文本转换为数字索引序列，方便模型操作。

## 1.读取数据集：

将文本进行过滤，如去掉标点符号、忽略大小写。
每个文本作为一行。
```
def read_text_file(file_name):  
    with open(file_name, "r") as f:  
        lines = f.readlines()  
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
```
## 2.词元化：

每个文本被拆分成以词元为单位的序列。**词元**是文本的基本单位。
```
def tokenize(lines, token="word"):  
    if token == 'word':  
        return [line.split() for line in lines]  
    elif token == 'char':  
        return [list(line) for line in lines]  
    else:  
        print("Unknown", token)
```
## 3.词表：

词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。
可以构建一个字典作为**词表(vocabulary)**，用来将token映射为从0开始的数字。
我们先将所有文档集合在一起，对**unique token**进行统计，得到的结果称为**语料库(corpus)**。
然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元\<unk>。

```
class Vocab:  
    """文本词表"""  
  
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):  
        if tokens is None:  
            tokens = []  
        if reserved_tokens is None:  
            reserved_tokens = []  
        # 按出现频率排序  
        counter = count_corpus(tokens)  
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],  
                                   reverse=True)  
        # 未知词元的索引为0  
        self.idx_to_token = ['<unk>'] + reserved_tokens  
        self.token_to_idx = {token: idx  
                             for idx, token in enumerate(self.idx_to_token)}  
        for token, freq in self._token_freqs:  
            if freq < min_freq:  
                break  
            if token not in self.token_to_idx:  
                self.idx_to_token.append(token)  
                self.token_to_idx[token] = len(self.idx_to_token) - 1  
  
    def __len__(self):  
        return len(self.idx_to_token)  
  
    def __getitem__(self, tokens):  
        if not isinstance(tokens, (list, tuple)):  
            return self.token_to_idx.get(tokens, self.unk)  
        return [self.__getitem__(token) for token in tokens]  
  
    def to_tokens(self, indices):  
        if not isinstance(indices, (list, tuple)):  
            return self.idx_to_token[indices]  
        return [self.idx_to_token[index] for index in indices]  
  
    @property  
    def unk(self):  # 未知词元的索引为0  
        return 0  
  
    @property  
    def token_freqs(self):  
        return self._token_freqs


def count_corpus(tokens):  # @save  
    """统计词元的频率"""  
    # 这里的tokens是1D列表或2D列表  
    if len(tokens) == 0 or isinstance(tokens[0], list):  
        # 将词元列表展平成一个列表  
        tokens = [token for line in tokens for token in line]  
    return collections.Counter(tokens)
```
## 4.加载语料库

将所有的文本展开到一个列表中并且返回**词表**。
```
def load_corpus(filename, max_tokens=-1, token="char"):  
    """返回数据集的词元索引列表和词表"""  
    lines = read_text_file(filename)  
    tokens = tokenize(lines, token)  
    vocab = Vocab(tokens)  
    # 所以将所有文本行展平到一个列表中  
    corpus = [vocab[token] for line in tokens for token in line]  
    if max_tokens > 0:  
        corpus = corpus[:max_tokens]  
    return corpus, vocab
```
